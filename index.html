<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Adaptive Token Reduction for Efficient Large  Multimodal Models">
  <meta name="keywords" content="Adaptive Token Reduction for Efficient Large  Multimodal Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning to Parallel</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="visualization/app.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"
    integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">

  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"
    integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6"
    crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
    integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.8.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }

  .author-block a {
    color: #008AD7;
    font-weight: normal;
  }

  /* Adjust the vertical alignment and font size of the superscript */
  .author-block a sup {
    vertical-align: baseline;
    position: relative;
    top: -0.3em;
    /* Adjusts the position slightly above the baseline */
    right: -0.1em;
    /* Adjusts the position slightly to the right */
    font-size: smaller;
    /* Makes the font size smaller if needed */
  }
</style>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning to Parallel<span class="is-size-2"><span
                  class="is-size-1"></span></h1>
            <h3 class="title is-2 publication-title">Accelerating Diffusion Large Language Models via Adaptive Parallel
              Decoding</h3>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                Wenrui Bao<sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/zhiben-chen">Zhiben Chen<sup>2,*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.danxurgb.net">Dan Xu<sup>3</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://42shawn.github.io/">Yuzhang Shang<sup>1,&dagger;</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>&dagger;</sup>Corresponding Author</span>
            </div>



            <div class="is-size-4 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">1. </b>University of Central Florida</span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">2. </b>Carnegie Mellon University</span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">3. </b>Hong Kong University of Sciences and Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/ims-kdks/Learning-to-Parallel-Decoding" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
            <img id="teaser" width="100%" src="images/introduction.png">
            <section class="section">
              <!-- visualization -->
              <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                  <h2 class="title is-3">Demo</h2>
                </div>
                <div class="decoder-visualization">
                  <p id="question" class="is-size-5 has-text-left">Loading question...</p>
            
                  <div class="controls">
                    <button id="playPause" class="button is-small is-link">Play</button>
                    <button id="replay" class="button is-small is-light" disabled>Replay</button>
                    <button id="prev" class="button is-small" disabled>â—€ Prev</button>
                    <button id="next" class="button is-small" disabled>Next â–¶</button>
                    <label>Speed: <input id="speed" type="range" min="50" max="2000" value="500" /></label>
                    <span id="speedLabel">500ms</span>
                  </div>
            
                  <div class="step">Step <strong id="stepNum">0</strong> / <strong id="total">0</strong></div>
                  <div id="tables-container"></div>
                </div>
              </div>
            </section>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-left">
          ðŸ”¥<span style="color: #ff3860">[NEW!]</span> <b>PruMerge+</b>: After supplementing with spatial tokens, we can further enhance the performance by a large margin. Four times of token reduction rate is achieved with lossless performance gap!
          <br><br>
          ðŸ”¥<span style="color: #ff3860"></span> We find that the visual tokens in current large multimodal models are spatially redundant, indicated by the sparse attention maps. 
          <br><br>
          ðŸ”¥<span style="color: #ff3860"></span> We propose <b>LLaVA-PruMerge</b> to first <i>prune</i> and then <i>merge</i> visual tokens, which can compress the visual tokens by <b>18</b> times  (14 times on MME/TextVQA) on average while maintaining <b>comparable performance</b>. 
        </h4>
      </div>
    </div>
  </section> -->

  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Autoregressive decoding in large language models (LLMs) requires \(\mathcal{O}(n)\) sequential steps for
              \(n\) tokens, fundamentally limiting inference throughput.
              Recent diffusion-based LLMs (dLLMs) enable parallel token generation through iterative denoising. However,
              current parallel decoding strategies rely on fixed, input-agnostic heuristics (e.g., confidence
              thresholds), which fail to adapt to input-specific characteristics, resulting in suboptimal speed-quality
              trade-offs across diverse NLP tasks.
              In this work, we explore a more flexible and dynamic approach to parallel decoding. We propose
              \(\textbf{Learning to Parallel Decode (Learn2PD)}\), a framework that trains a lightweight and adaptive
              filter model to predict, for each token position, whether the current prediction matches the final output.
              This learned filter approximates an oracle parallel decoding strategy that unmasks tokens only when
              correctly predicted. Importantly, the filter model is learned in a post-training manner, requiring only a
              small amount of computation to optimize it (minute-level GPU time). Additionally, we introduce
              \(\textbf{End-of-Text Prediction (EoTP)}\) to detect decoding completion at the end of sequence, avoiding
              redundant decoding of padding tokens.
              Experiments on the <a href="https://arxiv.org/pdf/2502.09992">LLaDA</a> benchmark demonstrate that our
              method achieves up to \(\textbf{22.58Ã—}\) speedup without any performance drop, and up to
              \(\textbf{57.51Ã—}\) when combined with KV-Cache.
              <!-- <ol type="1">
                <li><b>Multimodal Instruct Data</b>. <span style="font-size: 95%;">We present the first attempt to use <a href="https://openai.com/research/gpt-4">language-only GPT-4</a> to generate multimodal language-image instruction-following data. </span></li>
                <li><b>LLaVA Model</b>. <span style="font-size: 95%;">We introduce <it><b>LLaVA</b> (<b>L</b>arge <b>L</b>anguage-<b>a</b>nd-<b>V</b>ision <b>A</b>ssistant)</it>, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.</li>
                <li><b>Performance</b>. <span style="font-size: 95%;">Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset.
                  When fine-tuned on <a href="https://scienceqa.github.io/">Science QA</a>, the synergy of LLaVA and GPT-4  achieves a new state-of-the-art accuracy of 92.53%.</li>
                <li><b>Open-source</b>. <span style="font-size: 95%;">We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.</li>
              </ol>   -->
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="images/lightbulb.png"> Method</h2>
      </div>
      
      <h3>1. Learning to Parallel Decoding</h3>
      <img id="teaser" width="100%" src="images/overview.png">
      <div class="columns">
        <div class="column is-one-third method-text">
          <p>This strategy compares the predicted tokens with the reference answer and only remasks the tokens that do not match in these comparisons.</p>
        </div>
        <div class="column is-two-third method-text">
          <p>Use a trained \(Filter f_\theta\) that simulate the Extremely Greedy Parallel strategy after each decoding step to select tokens and decide whether to remask them.</p>
        </div>
      </div>
      
      <h3>2. End-of-Text Prediction</h3>
      <div class="columns">
      <img id="teaser" class="column is-one-third" src="images/eot.png">
      <div class="column is-two-third method-text">
        <p>
          Upon detection of an \([EoT]\) token, all subsequent tokens are assigned \([EoT]\) in parallel. 
          Since the \([EoT]\) tokens have no effect on other tokens beyond the first occurrence, we throw away those redundant \([EoT]\) tokens in the next diffusion step. 
          When the specified output length is very long (for example, 1024), this method can significantly reduce computation by dynamically reducing the input size during the diffusion process.
        </p>
      </div>
      </div>
    </div>
  </section>

  <section class="section">
    <!-- Performance -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/3515/3515174.png"> Performance</h2>
      </div>
      <p>
        Benchmark results on the LLaDA-8B-Instruct suite. 
        Each method was evaluated using two generation lengths (<b>256</b> and <b>1024</b>) across four datasets. 
        Performance is measured using three metrics: <b>TPS (tokens/sec)</b>, <b>speedup</b>, and <b>accuracy score</b>. 
        The highest throughput and speedup values for each configuration are highlighted in bold.
      </p>
      <img id="teaser" src="images/performance.png">
    </div>
  </section>
  
  <section class="section">
    <!-- Compatibility -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Compatibility with Key-Value Cache</h2>
      </div>
      <p>
        We further evaluate the compatibility of our approach with established Key-Value (KV) Cache techniques by integrating both <a href="https://nvlabs.github.io/Fast-dLLM/paper/fast_dllm.pdf">Dual Cache and Prefix Cache</a> strategies. 
        Experiments are conducted on GSM8K with a generation length of 1024 tokens. 
        As summarized in the following table, the baseline model (Learn2PD & EoTP) achieves a throughput of 12.26 TPS, a speed-up of 22.58Ã—, and an accuracy score of 79.83. 
        When augmented with the Dual Cache, the system attains substantially higher efficiency, reaching 31.23 TPS and a 57.51Ã— speedup, albeit with a slight decrease in accuracy (74.00). 
        Similarly, incorporating the Prefix Cache also brings noticeable improvements, yielding 14.79 TPS and a 27.23Ã— acceleration while maintaining a competitive score of 77.71. 
        These results confirm that our method is orthogonal to and fully compatible with standard KV caching mechanisms, demonstrating its ability to leverage such strategies to enhance inference efficiency.
      </p>
      <div class="columns is-centered">
        <img id="teaser" width="50%" src="images/kvcache.png">
      </div>
    </div>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{shang2024LLaVA-PruMerge,
          title={LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models},
          author={Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan},
          journal={ICCV},
          year={2025}
        }
      </code></pre>
    </div>
  </section> -->

  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under
        a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>. We thank the LLaDA team for giving us access to
        their models, and open-source projects.
      </p>

      <p>
        <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only.
        They are also restricted to uses that follow the license agreement of LLaDA, Fast-dLLM. The dataset
        is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used
        outside of research purposes.
      </p>

      <p>
        Related Links:
        <a href='https://arxiv.org/pdf/2502.09992'>[LLaDA]</a>
        <a href='https://nvlabs.github.io/Fast-dLLM/'>[Fast-dLLM]</a>
      </p>
    </div>
  </section>
  <script src="https://cdn.jsdelivr.net/npm/papaparse@5.4.1/papaparse.min.js"></script>
  <script src="visualization/app.js" data-base-path="visualization"></script>
</body>

</html>
